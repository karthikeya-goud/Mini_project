[[6.6389, 7.2368, 6.7857, 7.0, 7.2045, 7.4091, 8.7143], [0.0, 6.7105, 6.0238, 5.881, 6.1818, 6.5455, 6.7619], [8.3889, 8.4211, 7.9524, 6.2381, 7.2727, 7.5227, 8.2857], [5.75, 0.0, 6.3333, 6.2143, 5.8636, 6.7955, 0.0], [7.4444, 7.4211, 6.6667, 7.0952, 7.0909, 6.6818, 7.1429], [9.0, 9.4211, 7.7619, 7.9048, 8.3636, 8.2273, 8.3333], [7.3333, 7.7368, 7.1429, 7.4286, 7.0, 7.4545, 8.0476], [7.7222, 8.1053, 6.9048, 6.619, 7.1364, 7.0455, 7.5238], [7.3889, 8.2368, 7.0476, 7.2381, 6.5227, 6.8636, 7.3333], [7.9444, 8.2105, 6.4286, 6.7619, 6.5909, 7.2273, 7.1905], [7.2778, 8.1053, 7.0952, 7.2857, 7.0682, 7.1818, 7.3333], [6.8056, 7.3684, 6.5238, 7.1429, 6.3182, 6.7045, 0.0], [7.8333, 8.1579, 7.0476, 6.7143, 6.6591, 6.9773, 7.1905], [7.6389, 8.0, 7.381, 7.0952, 6.9545, 6.6591, 6.9524], [8.1111, 9.0, 8.0476, 7.7619, 7.5455, 7.4545, 8.0], [6.3056, 0.0, 0.0, 6.3333, 6.0455, 6.5682, 0.0], [6.4444, 6.7895, 6.0, 6.4286, 6.0455, 6.2727, 0.0], [7.0556, 7.6842, 6.9524, 6.6905, 6.6591, 7.2273, 0.0], [7.5278, 7.9737, 6.7619, 6.1905, 6.5909, 5.8636, 6.5238], [7.7778, 8.0, 6.9048, 7.4286, 7.2727, 6.9091, 7.7619], [7.6667, 8.5263, 6.9524, 7.2857, 7.1364, 7.2273, 8.0476], [7.8889, 8.7368, 7.8095, 7.4286, 7.8864, 7.5909, 8.2381], [0.0, 6.5526, 0.0, 0.0, 6.0455, 0.0, 0.0], [6.4722, 0.0, 0.0, 6.0952, 0.0, 0.0, 0.0], [7.3611, 7.4211, 7.3333, 7.0952, 6.9318, 0.0, 7.0476], [7.5278, 7.7368, 7.381, 6.9524, 6.7955, 6.9318, 0.0], [6.4722, 7.3684, 6.7143, 6.7619, 6.1818, 7.1136, 7.3333], [6.4722, 7.0, 6.1429, 5.9524, 5.7955, 0.0, 6.6667], [6.8333, 7.9737, 7.0, 7.3333, 7.3409, 6.8182, 8.0476], [0.0, 6.2895, 0.0, 0.0, 0.0, 0.0, 0.0], [8.1667, 8.7895, 6.8571, 7.0714, 7.2045, 7.6818, 7.3333], [7.4167, 7.8421, 7.6667, 7.4286, 7.7045, 8.3182, 8.2381], [7.2222, 8.5789, 7.0, 6.5714, 7.1364, 7.7727, 8.1905], [5.9167, 0.0, 0.0, 7.0476, 7.0, 7.3864, 0.0], [7.7222, 8.1579, 7.9524, 7.5238, 7.9545, 7.8182, 7.5238], [8.2778, 8.3684, 7.4286, 7.2857, 7.6818, 7.5, 0.0], [5.9167, 6.8947, 6.5714, 6.2143, 6.1136, 7.1591, 6.7619], [7.3333, 8.3684, 7.381, 7.5714, 7.6818, 7.8409, 8.5238], [6.4444, 7.1842, 6.5238, 6.619, 6.4545, 6.8182, 7.381], [5.9167, 6.5789, 5.7857, 5.8333, 5.7955, 6.1364, 0.0], [6.0556, 0.0, 6.0, 6.4286, 5.8409, 7.1136, 6.8095], [7.8333, 7.5789, 6.7619, 7.0, 6.75, 7.8636, 7.6667], [5.6667, 6.9474, 6.1905, 6.6667, 6.4091, 7.4318, 7.619], [6.7222, 7.3158, 6.6667, 6.6429, 7.3409, 7.7727, 7.619], [8.5, 8.6316, 8.0476, 7.381, 7.1591, 7.9545, 8.2381], [7.4444, 7.8421, 6.7143, 6.9048, 7.0682, 7.7727, 7.6667], [8.3889, 8.6316, 7.0, 7.9048, 7.6818, 7.9545, 8.0952], [6.8333, 7.9474, 6.2857, 6.5714, 6.7955, 7.8636, 7.7619], [6.8333, 7.5, 6.5714, 7.2381, 7.3409, 7.4091, 7.6667], [6.6111, 7.0, 6.1429, 6.1667, 6.7273, 6.3864, 6.8095], [6.3611, 6.6053, 6.4286, 5.8333, 5.7727, 0.0, 0.0], [5.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [6.5278, 6.9474, 6.1429, 6.4524, 6.6136, 6.6364, 7.381], [6.4444, 7.7895, 6.381, 6.9048, 6.9318, 6.8409, 6.9524], [7.1944, 8.3158, 7.2857, 7.4286, 7.0682, 7.2045, 8.0476], [6.0556, 6.6579, 6.381, 6.7143, 6.3182, 6.4773, 7.0476], [7.8889, 8.9474, 7.7143, 7.881, 7.5455, 7.4091, 8.0952], [7.3889, 9.2105, 7.5714, 8.0476, 7.9545, 7.7273, 8.5238], [6.3889, 7.6316, 6.1429, 6.9048, 6.5909, 7.0, 7.8571], [8.5, 8.3158, 7.381, 7.619, 7.4773, 7.4091, 8.0], [7.1111, 8.3684, 7.0476, 7.8095, 7.3409, 6.8636, 8.2381], [8.2778, 8.6316, 7.1429, 7.8095, 7.4091, 7.4091, 8.0952], [6.7778, 6.7105, 6.5714, 7.0476, 7.1364, 6.8182, 7.4286], [7.0, 8.4211, 6.381, 7.0476, 7.1364, 7.1818, 7.5714], [7.3889, 8.7368, 7.7143, 8.0476, 7.6818, 7.9773, 8.381], [5.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [6.9722, 6.7632, 6.1429, 6.3571, 5.9318, 6.3182, 0.0], [7.3889, 6.6316, 6.2857, 6.7619, 6.1818, 5.9318, 6.8571], [6.1389, 6.5263, 6.0476, 6.2143, 6.1818, 6.2727, 0.0], [7.6667, 8.1579, 6.7143, 7.0952, 7.2727, 7.1818, 8.0476], [7.7778, 6.8947, 6.1429, 6.8095, 6.4091, 6.5227, 7.0], [7.1111, 6.8684, 6.0952, 6.4286, 6.6591, 6.0682, 7.381], [8.1111, 8.8947, 7.1429, 7.381, 7.8182, 7.6364, 8.7143], [7.1111, 7.0526, 5.9048, 6.0952, 6.3864, 0.0, 0.0], [8.5, 8.5789, 7.5238, 7.0952, 6.8636, 7.0227, 7.619], [8.6667, 8.9474, 7.0476, 7.2381, 7.2727, 8.1818, 8.0476], [9.2222, 9.2105, 7.9524, 7.9048, 8.2273, 8.5909, 8.3333], [7.6667, 7.8421, 7.6667, 6.9048, 6.7273, 7.5909, 7.619], [6.8889, 7.2895, 6.1429, 6.3571, 6.0455, 6.8409, 6.9524], [7.7222, 6.9474, 6.8571, 6.619, 6.9318, 7.1591, 7.5714], [7.6667, 6.5, 5.8571, 6.5476, 6.75, 7.1136, 7.1905], [8.0278, 7.5789, 7.0476, 6.619, 7.0909, 7.4545, 7.1905], [6.4722, 6.6053, 6.5238, 6.4286, 6.4545, 7.5909, 7.2857], [0.0, 6.7632, 6.4286, 6.2143, 6.2727, 7.2045, 0.0], [7.7778, 6.5, 6.7619, 6.4762, 6.7273, 7.6364, 7.8571], [6.6667, 6.3947, 5.7857, 6.1667, 5.7727, 6.9318, 7.7143], [6.9722, 6.6316, 6.2857, 6.8571, 6.5909, 7.1136, 7.5238], [5.9722, 6.5526, 6.3333, 6.4286, 0.0, 6.8636, 6.7143], [8.4444, 7.2105, 6.8571, 6.5, 6.4545, 7.4318, 0.0], [6.9444, 7.1579, 6.9048, 6.7619, 6.6591, 7.6591, 8.1429], [0.0, 0.0, 6.0476, 0.0, 5.7273, 0.0, 0.0], [6.8889, 6.7368, 6.381, 6.7619, 6.4545, 7.3182, 0.0], [7.8889, 6.4737, 6.2857, 6.1667, 5.9318, 7.4318, 6.5714], [6.4722, 6.9474, 6.2857, 6.3571, 6.4545, 7.0909, 7.4286], [6.0556, 6.6053, 6.1905, 6.5, 6.7273, 0.0, 7.381], [7.8889, 6.5, 7.0476, 7.4286, 6.8182, 7.5227, 7.1905], [7.0556, 6.7105, 7.0, 7.0952, 6.1818, 7.6364, 8.0952], [7.9722, 6.5789, 6.1429, 5.5476, 0.0, 6.2045, 0.0], [7.5556, 6.7632, 6.1429, 6.0238, 6.1818, 6.9318, 0.0], [6.5278, 6.8158, 7.3333, 6.5476, 6.25, 7.4545, 0.0], [6.6111, 6.7895, 6.8095, 6.8333, 6.2727, 7.2045, 7.2857], [6.3611, 6.9211, 6.8571, 6.8095, 6.9318, 7.9545, 7.9048], [9.0, 8.0, 7.7143, 7.4286, 7.5455, 7.8636, 8.3333], [6.9167, 7.6316, 6.9524, 6.9048, 6.9318, 7.2727, 7.4762], [6.9167, 6.7895, 6.5714, 6.2143, 6.0, 6.5682, 6.619], [6.3333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [5.9167, 6.4737, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [5.6667, 0.0, 0.0, 6.0714, 5.5909, 6.1818, 0.0], [7.0556, 7.6842, 7.5238, 6.8095, 6.5909, 7.0455, 7.381], [6.75, 7.3421, 6.6667, 6.9524, 7.3409, 7.5, 7.7619], [6.5833, 7.2105, 6.0, 5.9524, 6.0455, 6.5227, 0.0], [8.2222, 8.0526, 6.9524, 7.2857, 7.2727, 7.5, 8.0], [8.0556, 8.0263, 7.4286, 7.7619, 7.0909, 7.9091, 7.9524], [6.1667, 0.0, 0.0, 6.0476, 6.1136, 6.6136, 7.5714], [7.6667, 7.9211, 6.5714, 6.0952, 6.5909, 6.25, 0.0], [5.6667, 0.0, 5.881, 5.5476, 5.5682, 0.0, 0.0], [8.4444, 7.6316, 6.381, 6.1429, 6.5227, 6.7955, 7.1429], [6.25, 0.0, 0.0, 6.2143, 6.7955, 6.9318, 6.8095], [6.1389, 6.9211, 5.9524, 5.9524, 5.9091, 6.4318, 0.0], [6.7778, 0.0, 6.0476, 5.9762, 6.1136, 0.0, 6.6667], [7.3611, 7.3947, 7.0, 6.5952, 6.0455, 6.9091, 7.2857], [7.6389, 7.5263, 6.9048, 7.381, 7.0682, 7.3636, 8.0476], [8.3333, 8.0526, 7.381, 7.7143, 7.6818, 7.5, 7.8571], [7.1944, 7.1053, 6.3333, 7.0476, 6.5227, 6.5455, 0.0], [5.9167, 7.0, 6.0, 5.9048, 5.7273, 0.0, 0.0], [6.5556, 6.7632, 6.7619, 6.0952, 6.3182, 0.0, 0.0], [0.0, 0.0, 5.9524, 5.7381, 5.7955, 0.0, 0.0], [0.0, 0.0, 0.0, 5.6667, 5.6591, 6.3636, 7.0476], [0.0, 0.0, 6.1905, 6.6905, 6.0455, 6.2727, 0.0], [0.0, 0.0, 6.0952, 7.1429, 6.7955, 6.7045, 0.0], [0.0, 0.0, 5.9286, 5.881, 0.0, 0.0, 0.0], [0.0, 0.0, 5.6429, 6.381, 5.9318, 0.0, 6.5714], [0.0, 0.0, 5.6429, 5.8095, 6.0455, 0.0, 0.0], [0.0, 0.0, 0.0, 6.1667, 5.7273, 6.4318, 0.0], [0.0, 0.0, 7.2381, 7.6667, 7.5455, 8.1818, 8.381], [0.0, 0.0, 0.0, 6.2857, 5.8636, 0.0, 6.9524], [0.0, 0.0, 0.0, 5.9524, 0.0, 0.0, 0.0]]






import sqlite3
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt

# Assume 'student_sgpas' is a list of lists containing 7 semesters of SGPA data for all students
# Example: student_sgpas = [[8.2, 8.1, 8.3, 8.4, 8.0, 8.3, 8.2], [7.5, 7.6, 7.7, 7.8, 7.5, 7.6, 7.4], ...]
student_sgpas = [
    [8.2, 8.1, 8.3, 8.4, 8.0, 8.3, 8.2],
    [7.5, 7.6, 7.7, 7.8, 7.5, 7.6, 7.4],
    # Add more students data as needed
]

# Convert the list of lists to a DataFrame
columns = ['Sem1_SGPA', 'Sem2_SGPA', 'Sem3_SGPA', 'Sem4_SGPA', 'Sem5_SGPA', 'Sem6_SGPA', 'Sem7_SGPA']
df = pd.DataFrame(student_sgpas, columns=columns)

# Step 1: Impute failed SGPA values (0 means failure)
def impute_failed_sgpas(row):
    failed_semesters = (row == 0).sum()  # Count how many semesters the student failed in
    if failed_semesters == 2:
        # Replace failed SGPA with mean if failed in exactly 2 semesters
        for i in range(len(row)):  # Assuming row[i] is SGPA for the i-th semester
            if row[i] == 0:
                row[i] = df.iloc[:, i].mean()  # Mean of the i-th semester across all students
    elif failed_semesters > 2:
        # Replace failed SGPA with median if failed in more than 2 semesters
        for i in range(len(row)):
            if row[i] == 0:
                row[i] = df.iloc[:, i].median()  # Median of the i-th semester across all students
    return row

# Apply the imputation function to each student (row)
df_imputed = df.apply(impute_failed_sgpas, axis=1)

# Step 2: Create trend features (change between consecutive semesters)
df_imputed['Trend_Sem7_Sem6'] = df_imputed['Sem7_SGPA'] - df_imputed['Sem6_SGPA']
df_imputed['Trend_Sem6_Sem5'] = df_imputed['Sem6_SGPA'] - df_imputed['Sem5_SGPA']
df_imputed['Trend_Sem5_Sem4'] = df_imputed['Sem5_SGPA'] - df_imputed['Sem4_SGPA']
df_imputed['Trend_Sem4_Sem3'] = df_imputed['Sem4_SGPA'] - df_imputed['Sem3_SGPA']
df_imputed['Trend_Sem3_Sem2'] = df_imputed['Sem3_SGPA'] - df_imputed['Sem2_SGPA']
df_imputed['Trend_Sem2_Sem1'] = df_imputed['Sem2_SGPA'] - df_imputed['Sem1_SGPA']

# Step 3: Aggregate Features (e.g., Average and Variance of SGPA)
df_imputed['Avg_SGPA'] = df_imputed[['Sem1_SGPA', 'Sem2_SGPA', 'Sem3_SGPA', 'Sem4_SGPA', 'Sem5_SGPA', 
                                      'Sem6_SGPA', 'Sem7_SGPA']].mean(axis=1)
df_imputed['Variance_SGPA'] = df_imputed[['Sem1_SGPA', 'Sem2_SGPA', 'Sem3_SGPA', 'Sem4_SGPA', 'Sem5_SGPA', 
                                          'Sem6_SGPA', 'Sem7_SGPA']].var(axis=1)

# Step 4: Prepare features (X) and target variable (y) for Sem8 prediction
X = df_imputed[['Sem1_SGPA', 'Sem2_SGPA', 'Sem3_SGPA', 'Sem4_SGPA', 'Sem5_SGPA', 'Sem6_SGPA', 'Sem7_SGPA', 
                'Trend_Sem7_Sem6', 'Trend_Sem6_Sem5', 'Trend_Sem5_Sem4', 'Trend_Sem4_Sem3', 'Trend_Sem3_Sem2', 
                'Trend_Sem2_Sem1', 'Avg_SGPA', 'Variance_SGPA']]  # Features
y = df_imputed['Sem8_SGPA']  # Target variable (Sem8 SGPA)

# Step 5: Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 6: Standardize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Step 7: Train the model (Random Forest)
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train_scaled, y_train)

# Step 8: Bootstrapping for Confidence Interval
def predict_with_confidence(model, X_new, n_iterations=100, confidence=0.95):
    predictions = []
    for _ in range(n_iterations):
        bootstrap_pred = model.predict(X_new)
        predictions.append(bootstrap_pred)
    
    predictions = np.array(predictions)
    lower_percentile = (1 - confidence) / 2 * 100
    upper_percentile = (1 + confidence) / 2 * 100
    
    lower_bound = np.percentile(predictions, lower_percentile, axis=0)
    upper_bound = np.percentile(predictions, upper_percentile, axis=0)
    
    return lower_bound, upper_bound

# Example new data (list of new student SGPA for prediction)
new_data = [[8.2, 8.1, 8.3, 8.4, 8.0, 8.3, 8.2, 0.1, 0.2, 0.1, 0.1, 0.3, 0.1, 8.1, 0.03]]  # Example student
new_data_scaled = scaler.transform(new_data)

# Predict the confidence interval
lower_bound, upper_bound = predict_with_confidence(model, new_data_scaled, n_iterations=1000)

# Output the prediction range
print(f"Predicted Sem8 SGPA: {lower_bound[0]:.2f} to {upper_bound[0]:.2f} (95% confidence interval)")

# Step 9: Evaluate the model performance
y_pred = model.predict(X_test_scaled)
mse = mean_squared_error(y_test, y_pred)
rmse = mse**0.5
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"Root Mean Squared Error: {rmse}")
print(f"R² Score: {r2}")

# Optionally, visualize feature importance using matplotlib
importances = model.feature_importances_
features = X.columns
indices = np.argsort(importances)[::-1]

plt.figure(figsize=(10, 6))
plt.title("Feature Importance")
plt.barh(range(len(importances)), importances[indices], align="center")
plt.yticks(range(len(importances)), [features[i] for i in indices])
plt.xlabel("Relative Importance")
plt.show()
